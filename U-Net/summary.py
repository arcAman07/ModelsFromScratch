from final_model import UNet

model = UNet(n_channels = 1, n_classes= 2, bilinear = False)
from torchvision import models
from torchsummary import summary

print(summary(model.cuda(),(1, 572, 572)))

"""

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 572, 572]             576
       BatchNorm2d-2         [-1, 64, 572, 572]             128
              ReLU-3         [-1, 64, 572, 572]               0
            Conv2d-4         [-1, 64, 572, 572]          36,864
       BatchNorm2d-5         [-1, 64, 572, 572]             128
              ReLU-6         [-1, 64, 572, 572]               0
        DoubleConv-7         [-1, 64, 572, 572]               0
         MaxPool2d-8         [-1, 64, 286, 286]               0
            Conv2d-9        [-1, 128, 286, 286]          73,728
      BatchNorm2d-10        [-1, 128, 286, 286]             256
             ReLU-11        [-1, 128, 286, 286]               0
           Conv2d-12        [-1, 128, 286, 286]         147,456
      BatchNorm2d-13        [-1, 128, 286, 286]             256
             ReLU-14        [-1, 128, 286, 286]               0
       DoubleConv-15        [-1, 128, 286, 286]               0
             Down-16        [-1, 128, 286, 286]               0
        MaxPool2d-17        [-1, 128, 143, 143]               0
           Conv2d-18        [-1, 256, 143, 143]         294,912
      BatchNorm2d-19        [-1, 256, 143, 143]             512
             ReLU-20        [-1, 256, 143, 143]               0
           Conv2d-21        [-1, 256, 143, 143]         589,824
      BatchNorm2d-22        [-1, 256, 143, 143]             512
             ReLU-23        [-1, 256, 143, 143]               0
       DoubleConv-24        [-1, 256, 143, 143]               0
             Down-25        [-1, 256, 143, 143]               0
        MaxPool2d-26          [-1, 256, 71, 71]               0
           Conv2d-27          [-1, 512, 71, 71]       1,179,648
      BatchNorm2d-28          [-1, 512, 71, 71]           1,024
             ReLU-29          [-1, 512, 71, 71]               0
           Conv2d-30          [-1, 512, 71, 71]       2,359,296
      BatchNorm2d-31          [-1, 512, 71, 71]           1,024
             ReLU-32          [-1, 512, 71, 71]               0
       DoubleConv-33          [-1, 512, 71, 71]               0
             Down-34          [-1, 512, 71, 71]               0
        MaxPool2d-35          [-1, 512, 35, 35]               0
           Conv2d-36         [-1, 1024, 35, 35]       4,718,592
      BatchNorm2d-37         [-1, 1024, 35, 35]           2,048
             ReLU-38         [-1, 1024, 35, 35]               0
           Conv2d-39         [-1, 1024, 35, 35]       9,437,184
      BatchNorm2d-40         [-1, 1024, 35, 35]           2,048
             ReLU-41         [-1, 1024, 35, 35]               0
       DoubleConv-42         [-1, 1024, 35, 35]               0
             Down-43         [-1, 1024, 35, 35]               0
  ConvTranspose2d-44          [-1, 512, 70, 70]       2,097,664
           Conv2d-45          [-1, 512, 71, 71]       4,718,592
      BatchNorm2d-46          [-1, 512, 71, 71]           1,024
             ReLU-47          [-1, 512, 71, 71]               0
           Conv2d-48          [-1, 512, 71, 71]       2,359,296
      BatchNorm2d-49          [-1, 512, 71, 71]           1,024
             ReLU-50          [-1, 512, 71, 71]               0
       DoubleConv-51          [-1, 512, 71, 71]               0
               Up-52          [-1, 512, 71, 71]               0
  ConvTranspose2d-53        [-1, 256, 142, 142]         524,544
           Conv2d-54        [-1, 256, 143, 143]       1,179,648
      BatchNorm2d-55        [-1, 256, 143, 143]             512
             ReLU-56        [-1, 256, 143, 143]               0
           Conv2d-57        [-1, 256, 143, 143]         589,824
      BatchNorm2d-58        [-1, 256, 143, 143]             512
             ReLU-59        [-1, 256, 143, 143]               0
       DoubleConv-60        [-1, 256, 143, 143]               0
               Up-61        [-1, 256, 143, 143]               0
  ConvTranspose2d-62        [-1, 128, 286, 286]         131,200
           Conv2d-63        [-1, 128, 286, 286]         294,912
      BatchNorm2d-64        [-1, 128, 286, 286]             256
             ReLU-65        [-1, 128, 286, 286]               0
           Conv2d-66        [-1, 128, 286, 286]         147,456
      BatchNorm2d-67        [-1, 128, 286, 286]             256
             ReLU-68        [-1, 128, 286, 286]               0
       DoubleConv-69        [-1, 128, 286, 286]               0
               Up-70        [-1, 128, 286, 286]               0
  ConvTranspose2d-71         [-1, 64, 572, 572]          32,832
           Conv2d-72         [-1, 64, 572, 572]          73,728
      BatchNorm2d-73         [-1, 64, 572, 572]             128
             ReLU-74         [-1, 64, 572, 572]               0
           Conv2d-75         [-1, 64, 572, 572]          36,864
      BatchNorm2d-76         [-1, 64, 572, 572]             128
             ReLU-77         [-1, 64, 572, 572]               0
       DoubleConv-78         [-1, 64, 572, 572]               0
               Up-79         [-1, 64, 572, 572]               0
           Conv2d-80          [-1, 2, 572, 572]             130
          OutConv-81          [-1, 2, 572, 572]               0
================================================================
Total params: 31,036,546
Trainable params: 31,036,546
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 1.25
Forward/backward pass size (MB): 5087.77
Params size (MB): 118.40
Estimated Total Size (MB): 5207.41
----------------------------------------------------------------
None

"""